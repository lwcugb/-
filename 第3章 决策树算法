'''

决策树优缺点：
（1）优点:计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
（2）缺点：可能会产生过度匹配问题。
（3）适用数据类型：数值型和标称型。
决策树的一般流程：
（1）收集数据：可以使用任何方法。
（2）准备数据：树构造算法只适用标称型数据，因此数值型数据必须离散化。
（3）分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
（4）训练算法：构造树的数据结构。
（5）测试算法：使用经验树计算错误率。
（6）使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。

'''
#以下代码命名为tree.py
#--------------------------------------------------------计算数据集的熵--------------------------------------------------------------
from math import log

def calcShannonEnt(dataSet):
    numEntries = len(dataSet)  #获取数据集的行数,dataSet.shape(0)也是获取行数
    labelCounts = {}   #设置字典的数据结构
    for featVec in dataSet:  #注意这种在数组中for循环的方法，featVec表示的是每一行的向量
        currentLabel = featVec[-1]  #获取特征向量的最后一列的标签
        if currentLabel not in labelCounts.keys():  #如果不存在keys()关键字
            labelCounts[currentLabel] = 0     #将当前标签/0键值对存入字典中
        labelCounts[currentLabel]+=1   #否则将当前标签对应的键值加1
    ShannonEnt = 0    #初始化熵为0
    for key in labelCounts:    
        prob = (labelCounts[key])/numEntries   #对于数据集中所有的分类类别，计算各个类别出现的频率
        ShanninEnt -=prob*log(prob,2)      #计算各个类别信息期望值 log(prob,2)表示以2为底的prob的对数
    return ShannonEnt
#可以自己创建一个简单的数据集来验证熵的计算是否正确，数据集中包含两个特征'no surfacing','flippers';数据的类标签有两个'yes','no'
def createDataSet():
    dataSet=[[1,1,'yes'],
            [1,1,'yes'],
            [1,0,'no'],
            [0,1,'no'],
            [0,1,'no']]
    labels=['no surfacing','flippers']
    return dataSet,labels  #返回数据集和类标签
#可以在Python命令提示符下输入以下命令，来检测输出的熵的值是否正确，另外需要说明的是，熵越高，那么混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大
#>>>import tree.py
#>>>myDat,label=tree.createDataSet()
#>>>tree.calcShannonEnt(myDat)





