'''
决策树优缺点：
（1）优点:计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
（2）缺点：可能会产生过度匹配问题。
（3）适用数据类型：数值型和标称型。
决策树的一般流程：
（1）收集数据：可以使用任何方法。
（2）准备数据：树构造算法只适用标称型数据，因此数值型数据必须离散化。
（3）分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
（4）训练算法：构造树的数据结构。
（5）测试算法：使用经验树计算错误率。
（6）使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。
'''

#以下代码的文件命名为tree.py
#--------------------------------------------------------计算数据集的熵--------------------------------------------------------------
from math import log

def calcShannonEnt(dataSet):
    numEntries = len(dataSet)  #获取数据集的行数,dataSet.shape(0)也是获取行数
    labelCounts = {}   #设置字典的数据结构
    for featVec in dataSet:  #注意这种在数组中for循环的方法，featVec表示的是每一行的向量
        currentLabel = featVec[-1]  #获取特征向量的最后一列的标签
        if currentLabel not in labelCounts.keys():  #如果不存在keys()关键字
            labelCounts[currentLabel] = 0     #将当前标签/0键值对存入字典中
        labelCounts[currentLabel]+=1   #否则将当前标签对应的键值加1
    ShannonEnt = 0    #初始化熵为0
    for key in labelCounts:    
        prob = (labelCounts[key])/numEntries   #对于数据集中所有的分类类别，计算各个类别出现的频率
        ShanninEnt -=prob*log(prob,2)      #计算各个类别信息期望值 log(prob,2)表示以2为底的prob的对数
    return ShannonEnt
#可以自己创建一个简单的数据集来验证熵的计算是否正确，数据集中包含两个特征'no surfacing','flippers';数据的类标签有两个'yes','no'
def createDataSet():
    dataSet=[[1,1,'yes'],
            [1,1,'yes'],
            [1,0,'no'],
            [0,1,'no'],
            [0,1,'no']]
    labels=['no surfacing','flippers']
    return dataSet,labels  #返回数据集和类标签
#可以在Python命令提示符下输入以下命令，来检测输出的熵的值是否正确，另外需要说明的是，熵越高，那么混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大
#>>>import tree.py
#>>>myDat,label=tree.createDataSet()
#>>>tree.calcShannonEnt(myDat)

#-----------------------------------------------按照某一个特征划分数据集------------------------------------------------------------
def splitDataSet(dataSet,axis,value):    #@dataSet:待划分的数据集 @axis:划分数据集的特征(注意这里输入的列的下标，而不是列) @value:特征的取值
	retDataSet = []   #需要说明的是,python语言传递参数列表时，传递的是列表的引用，如果在函数内部对列表对象进行修改，将会导致列表发生变化，为了不修改原始数据集，创建一个新的列表对象进行操作
	for featVec in dataSet:   #提取数据集的每一行的特征向量
		if featVec[axis] == value:    #如果该特征的取值为value
			reducedFeatVec=featVec[:axis]    #将特征向量的0~axis-1列存入列表reducedFeatVec
			reducedFeatVec.extend(featVec[axis+1:])   #将特征向量的axis+1~最后一列存入列表reducedFeatVec
			#extend()是将另外一个列表中的元素（以列表中元素为对象）添加到当前列表中，比如a=[1,2,3],b=[4,5,6],则a.extend(b)=[1,2,3,4,5,6]
			#append()是将另外一个列表（以列表为对象）添加到当前列表中，比如a=[1,2,3],b=[4,5,6],则a.extend(b)=[1,2,3,[4,5,6]]
			retDataSet.append(reducedFeatVec)
	return retDataSet
	
#----------------------------------------------选择最好的划分数据集的特征---------------------------------------------------------
#使用某一特征划分数据集，信息增益最大，则选择该特征作为最优特征
def chooseBestFeatureToSplit(dataSet):
    numFeatures=len(dataSet[0])-1   #获取数据集特征的数目(不包含最后一列的类标签),dataSet[0]表示选取第一行
    baseEntropy=calEnt(dataSet)     #计算未进行划分的信息熵
    #最优信息增益    最优特征
    bestInfoGain=0.0;bestFeature=-1
    
    for i in range(numFeatures):    #利用每一个特征分别对数据集进行划分，计算信息增益
       
        featList=[example[i] for example in dataSet]   #得到特征i的特征值列表example表示迭代dataSet的每一行，example[i]表示得到每一行第i列的特征值
       
        uniqueVals=set(featList)   #利用set集合的性质--元素的唯一性，得到特征i的取值，比如a=[1,2,1,3,3,4],那么set(a)=(1,2,3,4)
        #信息增益0.0
        newEntropy=0.0
       
        for value in uniqueVals:   #对特征的每一个取值，分别构建相应的分支
            subDataSet=splitDataSet(dataSet,i,value)  #根据特征i的取值将数据集进行划分为不同的子集,利用splitDataSet()获取特征取值Value分支包含的数据集
            prob=len(subDataSet)/float(len(dataSet))  #计算特征取值value对应子集占数据集的比例
            newEntropy+=prob*calEnt(subDataSet)       #计算占比*当前子集的信息熵,并进行累加得到总的信息熵
			
        #计算按此特征划分数据集的信息增益
        #公式特征A,数据集D
        #则H(D,A)=H(D)-H(D/A)
        infoGain=baseEntropy-newEntropy
		
        #比较此增益与当前保存的最大的信息增益
        if (infoGain>bestInfoGain):
           bestInfoGain=infoGain  #保存信息增益的最大值
           bestFeature=i          #相应地保存得到此最大增益的特征i
        
    return bestFeature            #返回最优特征
			
#-------------------------------------------------采用多数表决的方法完成分类--------------------------------------------------------
#当遍历完所有的特征属性后，类标签仍然不唯一(分支下仍有不同分类的实例)的时候需要用到以下代码，选取多数分类标签为该类标签
def majorityCnt(classList):
    classCount={}  #创建一个类标签的字典
    #遍历类标签列表中每一个元素
    for vote in classList:
    	if vote not in classCount.keys():   #如果元素不在字典中
        	classCount[vote]=0   #在字典中添加新的键值对
        
        classCount[vote]+=1      #否则，当前键对于的值加1
    #对字典中的键对应的值所在的列，按照又大到小进行排序(改方法在KNN算法里面也用到过)
    #@classCount.iteritems 得到列表对象
    #@key=operator.itemgetter(1) 获取列表对象的第二个域的值
    #@reverse=true 降序排序，默认是升序排序
    sortedClassCount=sorted(classCount.iteritems, key=operator.itemgetter(1),reverse=true)
    #返回出现次数最多的类标签
    return sortedClassCount[0][0]

#-----------------------------------------------通过递归的方式构建决策树的代码-------------------------------------------------------
def createTree(dataSet,labels):                        #这里的参数labels指的是各种特征的名称，比如书上的'no surfacing'和'flipper'
    classList=[example[-1] for example in dataSet]     #获取数据集中的最后一列的类标签，存入classList列表
    
    #通过count()函数获取类标签列表中第一个类标签的数目                                                
    if classList.count(classList[0])==len(classList):  #判断数目是否等于列表长度，相同表明所有类标签相同，属于同一类
        return classList[0]
    #遍历完所有的特征属性，此时数据集的列为1，即只有类标签列
    if len(dataSet[0])==1:
        #多数表决原则，确定类标签
        return majorityCnt(classList)
   
    bestFeat=chooseBestFeatureToSplit(dataSet)       #确定出当前最优的分类特征，返回列的下标
    bestFeatLabel=labels[bestFeat]                   #在特征标签列表中获取该特征对应的值，得到特征的名称
    myTree={bestFeatLabel:{}}                        #采用字典嵌套字典的方式，存储分类树信息。该类嵌套对于初学者可能不太熟悉。
                                                     #可以举一个简单的例子如 a={'小红':{'身高':170,'体重':60},'小明':{'身高':175,'体重':70}}
    ##此位置书上写的有误，书上为del(labels[bestFeat])    #这样我们取值或者赋值的时候就可以用 a[0][0]来获得小红的身高值
    ##相当于操作原始列表内容，导致原始列表内容发生改变
    ##按此运行程序，报错'no surfacing'is not in list
    ##以下代码已改正
   
    subLabels=labels[:]                                    #复制当前特征标签列表，防止改变原始列表的内容
    del(subLabels[bestFeat])                               #删除属性列表中当前分类数据集特征
    featValues=[example[bestFeat] for example in dataSet]  #获取数据集中最优特征所在列
    uniqueVals=set(featValues)                             #采用set集合性质，获取特征的所有的唯一取值
	
    #遍历每一个特征取值
    for value in uniqueVals:
        #采用递归的方法利用该特征对数据集进行分类
        #@bestFeatLabel 分类特征的特征标签值
        #@dataSet 要分类的数据集
        #@bestFeat 分类特征的标称值,即特征列的下标
        #@value 标称型特征的取值(即把文本转化成数字表示，如书中用'0'代替'否'，用'1'代替'是')
        #@subLabels 去除分类特征后的子特征标签列表
        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels)
    return myTree
			
			
			
			
			



